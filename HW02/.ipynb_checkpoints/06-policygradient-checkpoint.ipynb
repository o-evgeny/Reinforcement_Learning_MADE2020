{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import cProfile\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('figure', **{'dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, exptuple):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = exptuple\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "       \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, layer_size=256):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, layer_size)\n",
    "        self.l2 = nn.Linear(layer_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(xs, labels):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.xlabel('Номер эпизода')\n",
    "    plt.ylabel('Число шагов')\n",
    "    for i,x in enumerate(xs):\n",
    "        plt.plot(x, label=labels[i])\n",
    "    plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleDQN():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.model = Network()\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.001)\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        \n",
    "        self.gamma = 0.8\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        self.eps_init, self.eps_final, self.eps_decay = 0.9, 0.05, 200\n",
    "        self.num_step = 0\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        return self.model(state).data.max(1)[1].view(1, 1)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        self.num_step += 1\n",
    "        eps_threshold = self.eps_final + (self.eps_init - self.eps_final) * math.exp(-1. * self.num_step / self.eps_decay)\n",
    "        if sample > eps_threshold:\n",
    "            return self.select_greedy_action(state)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(2)]], dtype=torch.int64)\n",
    "        \n",
    "    def run_episode(self, e=0, do_learning=True, greedy=False, render=False):\n",
    "        state, num_step = self.env.reset(), 0\n",
    "        while True:\n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            state_tensor = torch.tensor([state], dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                if greedy:\n",
    "                    action = self.select_greedy_action(state_tensor)\n",
    "                else:\n",
    "                    action = self.select_action(state_tensor)\n",
    "            next_state, reward, done, _ = self.env.step(action.numpy()[0][0])\n",
    "            next_state_tensor = torch.tensor([next_state], dtype=torch.float32)\n",
    "\n",
    "            if done:\n",
    "                reward = -1\n",
    "\n",
    "            transition = (state_tensor, action, next_state_tensor, torch.tensor([reward], dtype=torch.float32))\n",
    "            self.memory.store(transition)\n",
    "\n",
    "            if do_learning:\n",
    "                self.learn()\n",
    "\n",
    "            state = next_state\n",
    "            num_step += 1\n",
    "\n",
    "            if done:\n",
    "                print(\"\\tepisode %d finished after %d steps\" % (e, num_step))\n",
    "                self.episode_durations.append(num_step)\n",
    "                break\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # берём мини-батч из памяти\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "\n",
    "        batch_state = Variable(torch.cat(batch_state))\n",
    "        batch_action = Variable(torch.cat(batch_action))\n",
    "        batch_reward = Variable(torch.cat(batch_reward))\n",
    "        batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\n",
    "        # считаем значения функции Q\n",
    "        Q = self.model(batch_state).gather(1, batch_action).reshape([self.batch_size])\n",
    "\n",
    "        # оцениваем ожидаемые значения после этого действия\n",
    "        Qmax = self.model(batch_next_state).detach().max(1)[0]\n",
    "        Qnext = batch_reward + (self.gamma * Qmax)\n",
    "\n",
    "        # и хотим, чтобы Q было похоже на Qnext -- это и есть суть Q-обучения\n",
    "        loss = F.smooth_l1_loss(Q, Qnext)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn = CartpoleDQN()\n",
    "\n",
    "print(\"%s\\tStarting training for 300 episodes...\" % (datetime.now().time()))\n",
    "for e in range(300):\n",
    "    dqn.run_episode(e)\n",
    "print(\"%s\\t\\t...done!\" % (datetime.now().time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_durations([dqn.episode_durations], ['DQN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, layer_size=256):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, layer_size)\n",
    "        self.l2 = nn.Linear(layer_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.softmax(self.l2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpolePolicyGradient():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.model = PolicyNetwork()\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), 0.001)\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        \n",
    "        self.gamma = 0.8\n",
    "        \n",
    "    def discount_rewards(self, r):\n",
    "        '''выдаём дисконтированные награды'''\n",
    "        discounted_r = torch.zeros(r.size())\n",
    "        running_add = 0\n",
    "        for t in reversed(range(len(r))):\n",
    "            running_add = running_add * self.gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "\n",
    "        return discounted_r\n",
    "\n",
    "    def run_episode(self, e=0):\n",
    "        state = self.env.reset()\n",
    "        reward_sum = 0\n",
    "        xs = torch.tensor([], dtype=torch.float32)\n",
    "        ys = torch.tensor([], dtype=torch.float32)\n",
    "        rewards = torch.tensor([], dtype=torch.float32)\n",
    "        num_step = 0\n",
    "\n",
    "        while True:\n",
    "            x = torch.tensor([state], dtype=torch.float32)\n",
    "            xs = torch.cat([xs, x])\n",
    "\n",
    "            # считаем вероятности действий и выбираем одно из двух\n",
    "            action_prob = self.model(Variable(x))\n",
    "            action = 0 if random.random() < action_prob.data[0][0] else 1\n",
    "\n",
    "            y = torch.tensor([[1, 0]] if action == 0 else [[0, 1]], dtype=torch.float32)\n",
    "            ys = torch.cat([ys, y])\n",
    "\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            rewards = torch.cat([rewards, torch.tensor([[reward]], dtype=torch.float32)])\n",
    "            reward_sum += reward\n",
    "            num_step += 1\n",
    "\n",
    "            if done or num_step >= 500:\n",
    "                # считаем дисконтированные награды\n",
    "                targets = self.discount_rewards(rewards)\n",
    "                \n",
    "                # нормализуем награды (это baseline)\n",
    "                targets = (targets - targets.mean())/(targets.std() + 1e-6)\n",
    "                \n",
    "                loss = self.learn(xs, ys, targets)\n",
    "                print(\"\\tepisode %d finished after %d steps\" % (e, num_step))\n",
    "                self.episode_durations.append(num_step)\n",
    "                break\n",
    "\n",
    "    def learn(self, x, y, targets):\n",
    "        # предсказания вероятностей действий\n",
    "        action_pred = self.model(Variable(x))\n",
    "        y = Variable(y, requires_grad=True)\n",
    "        targets = Variable(targets)\n",
    "        log_lik = -y * torch.log(action_pred)\n",
    "        log_lik_adv = log_lik * targets\n",
    "        loss = torch.sum(log_lik_adv, 1).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pg = CartpolePolicyGradient()\n",
    "\n",
    "print(\"%s\\tStarting training for 300 episodes...\" % (datetime.now().time()))\n",
    "for e in range(300):\n",
    "    pg.run_episode(e)\n",
    "print(\"%s\\t\\t...done!\" % (datetime.now().time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_durations([dqn.episode_durations, pg.episode_durations], ['DQN', 'Policy Gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем крестики-нолики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WIN = 3, 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN):\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_win = n_win\n",
    "\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.gameOver = False\n",
    "        self.boardHash = None\n",
    "        # ход первого игрока\n",
    "        self.curTurn = 1\n",
    "        self.emptySpaces = None\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "        self.boardHash = None\n",
    "\n",
    "    def getHash(self):\n",
    "        if self.boardHash is None:\n",
    "            self.boardHash = ''.join(['%s' % (x+1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
    "        return self.boardHash\n",
    "\n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_marks, cur_p = np.where(self.board == self.curTurn), self.curTurn\n",
    "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
    "#             print((i,j))\n",
    "            win = False\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
    "                    win = True\n",
    "            if not win:\n",
    "                if j <= self.n_cols - self.n_win:\n",
    "                    if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                    if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
    "                    if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----'*(self.n_cols)+'-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----'*(self.n_cols)+'-')\n",
    "\n",
    "    def getState(self):\n",
    "        return (self.getHash(), self.getEmptySpaces(), self.curTurn)\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return ( int(action_int / self.n_cols), int(action_int % self.n_cols))\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            return self.getState(), -10, True, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.boardHash = None\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_board(env, pi, showtext=True, verbose=True, fontq=20, fontx=60):\n",
    "    '''Рисуем доску с оценками из стратегии pi'''\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    X, Y = np.meshgrid(np.arange(0, env.n_rows), np.arange(0, env.n_rows))\n",
    "    Z = np.zeros((env.n_rows, env.n_cols)) + .01\n",
    "    s, actions = env.getHash(), env.getEmptySpaces()\n",
    "    if pi is not None and s in pi.Q:\n",
    "        for i, a in enumerate(actions):\n",
    "            Z[a[0], a[1]] = pi.Q[s][i]\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    surf = ax.imshow(Z, cmap=plt.get_cmap('Accent', 10), vmin=-1, vmax=1)\n",
    "    if showtext:\n",
    "        for i,a in enumerate(actions):\n",
    "            if pi is not None and s in pi.Q:\n",
    "                ax.text( a[1] , a[0] , \"%.3f\" % pi.Q[s][i], fontsize=fontq, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "#             else:\n",
    "#                 ax.text( a[1] , a[0] , \"???\", fontsize=fontq, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "    for i in range(env.n_rows):\n",
    "        for j in range(env.n_cols):\n",
    "            if env.board[i, j] == -1:\n",
    "                ax.text(j, i, \"O\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "            if env.board[i, j] == 1:\n",
    "                ax.text(j, i, \"X\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "    cbar = plt.colorbar(surf, ticks=[0, 1])\n",
    "    ax.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def get_and_print_move(env, pi, s, actions, random=False, verbose=True, fontq=20, fontx=60):\n",
    "    '''Делаем ход, рисуем доску'''\n",
    "    plot_board(env, pi, fontq=fontq, fontx=fontx)\n",
    "    if verbose and (pi is not None):\n",
    "        if s in pi.Q:\n",
    "            for i,a in enumerate(actions):\n",
    "                print(i, a, pi.Q[s][i])\n",
    "        else:\n",
    "            print(\"Стратегия не знает, что делать...\")\n",
    "    if random:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return pi.getActionGreedy(s, len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_game(env, pi1, pi2, random_crosses=False, random_naughts=True, verbose=True, fontq=20, fontx=60):\n",
    "    '''Играем тестовую партию между стратегиями или со случайными ходами, рисуем ход игры'''\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        s, actions = env.getHash(), env.getEmptySpaces()\n",
    "        if env.curTurn == 1:\n",
    "            a = get_and_print_move(env, pi1, s, actions, random=random_crosses, verbose=verbose, fontq=fontq, fontx=fontx)\n",
    "        else:\n",
    "            a = get_and_print_move(env, pi2, s, actions, random=random_naughts, verbose=verbose, fontq=fontq, fontx=fontx)\n",
    "        observation, reward, done, info = env.step(actions[a])\n",
    "        if reward == 1:\n",
    "            print(\"Крестики выиграли!\")\n",
    "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)\n",
    "        if reward == -1:\n",
    "            print(\"Нолики выиграли!\")\n",
    "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = TicTacToe(n_rows=15, n_cols=15, n_win=5)\n",
    "plot_test_game(env, None, None, random_crosses=True, random_naughts=True, verbose=True, fontx=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
