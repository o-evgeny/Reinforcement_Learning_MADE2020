{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание: https://docs.google.com/document/d/18bJyle9I2GCnxb4sygRHswLA_k6lLlLdW_Gatd_GRwM  \n",
    "Доп. материалы:  \n",
    "https://incompleteideas.net/book/bookdraft2017nov5.pdf  \n",
    "https://github.com/udacity/deep-reinforcement-learning/blob/master/cheatsheet/cheatsheet.pdf  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced DL and RL: Домашнее задание 2  \n",
    "Второе домашнее задание опять посвящено игре; его базовая часть, надеюсь, не слишком большая, но я добавил опциональную часть, которая, думаю, должна быть достаточно интересной для любого слушателя. Как обычно, в качестве решения ожидается ссылка на jupyter-ноутбук на вашем github (или публичный, или с доступом для snikolenko); ссылку обязательно нужно прислать в виде сданного домашнего задания на портале Академии. Любые комментарии, новые идеи и рассуждения на тему, как всегда, категорически приветствуются.  \n",
    "\n",
    "## Часть первая: крестики-нолики при помощи Q-обучения  \n",
    "В коде, прилагающемся к последней лекции про обучение с подкреплением, реализован Environment для крестиков-ноликов, в котором можно при инициализации указывать разные размеры доски и условия победы, а также функции для рисования, в том числе с указанием оценки различных действий. С этим окружением все задания и связаны.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import sys\n",
    "from time import sleep\n",
    "import tqdm.notebook as tq\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import cProfile\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('figure', **{'dpi': 300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS, N_COLS, N_WIN = 3, 3, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я допилил класс TicTacToe под себя. Добавил дополнительные методы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe(gym.Env):\n",
    "    def __init__(self, n_rows=N_ROWS, n_cols=N_COLS, n_win=N_WIN):\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_win = n_win\n",
    "\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.gameOver = False\n",
    "        self.boardHash = None\n",
    "        # ход первого игрока\n",
    "        self.curTurn = 1\n",
    "        self.emptySpaces = None\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def getNumStates(self):\n",
    "        return self.n_rows * self.n_cols * 3\n",
    "    \n",
    "    def getTotalNumActions(self):\n",
    "        return self.n_rows * self.n_cols\n",
    "\n",
    "    def getEmptySpaces(self):\n",
    "        if self.emptySpaces is None:\n",
    "            res = np.where(self.board == 0)\n",
    "            self.emptySpaces = np.array([ (i, j) for i,j in zip(res[0], res[1]) ])\n",
    "        return self.emptySpaces\n",
    "\n",
    "    def makeMove(self, player, i, j):\n",
    "        self.board[i, j] = player\n",
    "        self.emptySpaces = None\n",
    "        self.boardHash = None\n",
    "\n",
    "    def getHash(self):\n",
    "        if self.boardHash is None:\n",
    "            self.boardHash = ''.join(['%s' % (x+1) for x in self.board.reshape(self.n_rows * self.n_cols)])\n",
    "        return self.boardHash\n",
    "\n",
    "    def isTerminal(self):\n",
    "        # проверим, не закончилась ли игра\n",
    "        cur_marks, cur_p = np.where(self.board == self.curTurn), self.curTurn\n",
    "        for i,j in zip(cur_marks[0], cur_marks[1]):\n",
    "            # print((i,j))\n",
    "            win = False\n",
    "            if i <= self.n_rows - self.n_win:\n",
    "                if np.all(self.board[i:i+self.n_win, j] == cur_p):\n",
    "                    win = True\n",
    "            if not win:\n",
    "                if j <= self.n_cols - self.n_win:\n",
    "                    if np.all(self.board[i,j:j+self.n_win] == cur_p):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j <= self.n_cols - self.n_win:\n",
    "                    if np.all(np.array([ self.board[i+k,j+k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if not win:\n",
    "                if i <= self.n_rows - self.n_win and j >= self.n_win-1:\n",
    "                    if np.all(np.array([ self.board[i+k,j-k] == cur_p for k in range(self.n_win) ])):\n",
    "                        win = True\n",
    "            if win:\n",
    "                self.gameOver = True\n",
    "                return self.curTurn\n",
    "\n",
    "        if len(self.getEmptySpaces()) == 0:\n",
    "            self.gameOver = True\n",
    "            return 0\n",
    "\n",
    "        self.gameOver = False\n",
    "        return None\n",
    "\n",
    "    def printBoard(self):\n",
    "        for i in range(0, self.n_rows):\n",
    "            print('----'*(self.n_cols)+'-')\n",
    "            out = '| '\n",
    "            for j in range(0, self.n_cols):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('----'*(self.n_cols)+'-')\n",
    "\n",
    "    def getState(self):\n",
    "        return (self.getHash(), self.getEmptySpaces(), self.curTurn)\n",
    "\n",
    "    def action_from_int(self, action_int):\n",
    "        return [int(action_int // self.n_cols), int(action_int % self.n_cols)]\n",
    "\n",
    "    def int_from_action(self, action):\n",
    "        return action[0] * self.n_cols + action[1]\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.board[action[0], action[1]] != 0:\n",
    "            return self.getState(), -10, True, {}\n",
    "        self.makeMove(self.curTurn, action[0], action[1])\n",
    "        reward = self.isTerminal()\n",
    "        self.curTurn = -self.curTurn\n",
    "        return self.getState(), 0 if reward is None else reward, reward is not None, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
    "        self.boardHash = None\n",
    "        self.gameOver = False\n",
    "        self.emptySpaces = None\n",
    "        self.curTurn = 1\n",
    "        return self.getState()\n",
    "    \n",
    "    def random_available_action(self):\n",
    "        return random.choice(self.getEmptySpaces())\n",
    "    \n",
    "    def random_available_action_no(self):\n",
    "        return self.int_from_action(self.random_available_action())\n",
    "    \n",
    "    def is_available_action(self, action):\n",
    "        # print(action, self.getEmptySpaces().tolist())\n",
    "        return action in self.getEmptySpaces().tolist()\n",
    "    \n",
    "    def is_available_action_int(self, action_no):\n",
    "        return self.is_available_action(self.action_from_int(action_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_board(env, pi, showtext=True, verbose=True, fontq=20, fontx=60):\n",
    "    '''Рисуем доску с оценками из стратегии pi'''\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    X, Y = np.meshgrid(np.arange(0, env.n_rows), np.arange(0, env.n_rows))\n",
    "    Z = np.zeros((env.n_rows, env.n_cols)) + .01\n",
    "    s, actions = env.getHash(), env.getEmptySpaces()\n",
    "    if pi is not None and s in pi.Q:\n",
    "        for i, a in enumerate(actions):\n",
    "            Z[a[0], a[1]] = pi.Q[s][i]\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    surf = ax.imshow(Z, cmap=plt.get_cmap('Accent', 10), vmin=-1, vmax=1)\n",
    "    if showtext:\n",
    "        for i,a in enumerate(actions):\n",
    "            if pi is not None and s in pi.Q:\n",
    "                ax.text( a[1] , a[0] , \"%.3f\" % pi.Q[s][i], fontsize=fontq, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "#             else:\n",
    "#                 ax.text( a[1] , a[0] , \"???\", fontsize=fontq, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "    for i in range(env.n_rows):\n",
    "        for j in range(env.n_cols):\n",
    "            if env.board[i, j] == -1:\n",
    "                ax.text(j, i, \"O\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "            if env.board[i, j] == 1:\n",
    "                ax.text(j, i, \"X\", fontsize=fontx, horizontalalignment='center', verticalalignment='center', color=\"w\" )\n",
    "    cbar = plt.colorbar(surf, ticks=[0, 1])\n",
    "    ax.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "def get_and_print_move(env, pi, s, actions, random=False, verbose=True, fontq=20, fontx=60):\n",
    "    '''Делаем ход, рисуем доску'''\n",
    "    plot_board(env, pi, fontq=fontq, fontx=fontx)\n",
    "    if verbose and (pi is not None):\n",
    "        if s in pi.Q:\n",
    "            for i,a in enumerate(actions):\n",
    "                print(i, a, pi.Q[s][i])\n",
    "        else:\n",
    "            print(\"Стратегия не знает, что делать...\")\n",
    "    if random:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return pi.getActionGreedy(s, len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_game(env, pi1, pi2, random_crosses=False, random_naughts=True, verbose=True, fontq=20, fontx=60):\n",
    "    '''Играем тестовую партию между стратегиями или со случайными ходами, рисуем ход игры'''\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        s, actions = env.getHash(), env.getEmptySpaces()\n",
    "        if env.curTurn == 1:\n",
    "            a = get_and_print_move(env, pi1, s, actions, random=random_crosses, verbose=verbose, fontq=fontq, fontx=fontx)\n",
    "        else:\n",
    "            a = get_and_print_move(env, pi2, s, actions, random=random_naughts, verbose=verbose, fontq=fontq, fontx=fontx)\n",
    "        observation, reward, done, info = env.step(actions[a])\n",
    "        if reward == 1:\n",
    "            print(\"Крестики выиграли!\")\n",
    "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)\n",
    "        if reward == -1:\n",
    "            print(\"Нолики выиграли!\")\n",
    "            plot_board(env, None, showtext=False, fontq=fontq, fontx=fontx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "plot_test_game(env, None, None, random_crosses=True, random_naughts=True, verbose=True, fontx=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично. Игровая среда работает, давайте проведём случайную игру, посмотрим, как записываются состояния, вознаграждения и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "for i in range(9):\n",
    "    print(env.step(env.random_available_action()))\n",
    "print(env.step((0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте обычное (табличное) Q-обучение. Обучите стратегии крестиков и ноликов для доски 3х3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Q-learning\")](https://hsto.org/webt/wf/6x/fi/wf6xfiyazgu0echvfsw8d9-oly4.png \"Q-learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_strategy(env, epsilon, Q):\n",
    "    \"\"\"Эпсилон-жадная стратегия.\n",
    "    С вероятностью эпсилон выбирает случайное действие, иначе - наиболее выгодное действие.\n",
    "    \n",
    "    Аргументы:\n",
    "    env -- Окружение.\n",
    "    epsilon -- Вероятность случайного действия.\n",
    "    Q -- Выгода от действий в каждом состоянии dict: key - state, value - list values of actions.\n",
    "    \"\"\"\n",
    "    state = env.getState()\n",
    "    state = (state[0], state[2])\n",
    "    if random.random() > epsilon:\n",
    "        available = [(i, v) for i, v in enumerate(Q[state]) if env.is_available_action_int(i)]\n",
    "        # print(available)\n",
    "        best_available = list(filter(lambda x: x[1] == max(available, key=lambda x: x[1])[1], available))\n",
    "        best_action = list(env.action_from_int(random.choice(best_available)[0]))\n",
    "        # print(\"best action\", best_action)\n",
    "        return best_action\n",
    "    else:\n",
    "        # print(\"random action\")\n",
    "        return env.random_available_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = lambda env: eps_greedy_strategy(env, 1.0, {})\n",
    "best_policy = lambda env, Q: eps_greedy_strategy(env, 0.0, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_generator(eps_start=1.0, eps_min=0.01, decay=0.99999):\n",
    "    \"\"\"Генератор элементов убывающей геометрической прогрессии с ограничением значения минимального элемента.\n",
    "    \n",
    "    Аргументы:\n",
    "    eps_start -- Первый элемент последовательности.\n",
    "    eps_min -- Минимальное значение элементов.\n",
    "    decay -- Знаменатель прогрессии.\n",
    "    \"\"\"\n",
    "    epsilon = eps_start\n",
    "    yield epsilon\n",
    "    command = None\n",
    "    while True:\n",
    "        if command == \"restart\":  # Начать сначала\n",
    "            epsilon = eps_start\n",
    "            yield epsilon\n",
    "        else:\n",
    "            epsilon = max(epsilon*decay, eps_min)  # Новый член последовательности\n",
    "        command = (yield epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, my_policy, opponent_policy, verbose=False):\n",
    "    env.reset()\n",
    "    if verbose:\n",
    "        env.printBoard()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = my_policy(env)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if verbose:\n",
    "            env.printBoard()\n",
    "        if done:\n",
    "            break\n",
    "        action = opponent_policy(env)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if verbose:\n",
    "            env.printBoard()\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_quality(env, my_policy, opponent_policy=random_policy, n_episodes=10000):\n",
    "    reward = 0\n",
    "    for i in range(n_episodes):\n",
    "        reward += play_episode(env, my_policy, opponent_policy)\n",
    "    return reward / n_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как взяться за обучение, нужно подумать, против кого мы будем учиться играть, как будем оценивать качество, насколько жадным нужно быть при выборе действий и т.д.?  \n",
    "\n",
    "Против кого играем?  \n",
    "Можно было бы играть  \n",
    "* против случайной стратегии;  \n",
    "* против стратегии, которая параллельно обучается играть за другого игрока;  \n",
    "* против какой-то фиксированной стратегии (эвристики, шаблоны и т.д.)  \n",
    "Реализация третьего варианта нетривиальна, а первые два кажутся неплохими и простыми.  \n",
    "\n",
    "Как оценивать качество игры?  \n",
    "Очевидно, что для оценки качества надо сыграть много партий. Но с кем? Хотелось бы играть с таким игроком, который будет выискивать слабые места в стратегии противника, создавать неожиданные ситуации. Но реализовать такое - непростая задача. Если играть с какой-то фиксированной жёсткой стратегией, то кажется, что мы очень скудную оценку дадим, потому что оценим качество игры только в маленьком наборе ситуаций, которые эта стратегия умеет создавать, а это не совсем то, что хочется. Хочется, чтобы обученная стратегия умела действовать в большом количестве возможных ситуаций. Поэтому я решил, что качество буду оценивать, играя против случайной стратегии.  \n",
    "Нужно заметить, что если я учусь играть с одной стратегией, а качество оцениваю, играя с другой стратегией, то игра будет не самая лучшая, а оценка заниженная, но я думаю, это не страшно, так даже интереснее.  \n",
    "Так же нужно понимать, что при увеличении размеров доски и длины выстраиваемой линии случайная стратегия будет играть всё хуже и хуже.  \n",
    "\n",
    "Насколько много надо исследовать?  \n",
    "Мне кажется, в крестиках ноликах нужно исследовать много. Нужно за время обучения попытаться посмотреть, что будет в самых разных ситуациях. Если мы будем играть очень жадно против таких же жадных игроков, то мы изучим очень мало игровых состояний.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании я решил параллельно учиться играть за обоих игроков и оценивать качество игры, играя со случайной стартегией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(env, gamma=0.8, learning_rate=0.001, n_episodes=100000):\n",
    "    Q = defaultdict(lambda: np.zeros(env.getTotalNumActions()))\n",
    "    \n",
    "    quality = []\n",
    "    my_policy = lambda env: eps_greedy_strategy(env, 0, Q)\n",
    "    quality.append(policy_quality(env, my_policy, n_episodes=n_episodes))\n",
    "    \n",
    "    eps_gen = eps_generator(eps_start=1.0, eps_min=0.01, decay=0.01**(1/n_episodes))\n",
    "    \n",
    "    pbar = tq.tqdm(total=n_episodes, desc='Episode', file=sys.stdout)\n",
    "    for i in range(n_episodes):\n",
    "        eps = next(eps_gen)\n",
    "        state = env.reset()\n",
    "        state = (state[0], state[2])\n",
    "        \n",
    "        reward_coef = 1.0\n",
    "        if np.random.sample() > 0.5:\n",
    "            # Other player\n",
    "            action_other = eps_greedy_strategy(env, eps, Q)\n",
    "            action_no_other = env.int_from_action(action_other)\n",
    "            next_state_other, reward_other, done_other, _ = env.step(action_other)\n",
    "            assert reward_other != -10\n",
    "            state = (next_state_other[0], next_state_other[2])\n",
    "            reward_coef = -reward_coef\n",
    "        \n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # While episode is not over\n",
    "        while not done:\n",
    "            # Choose action\n",
    "            action = eps_greedy_strategy(env, eps, Q)\n",
    "            action_no = env.int_from_action(action)\n",
    "            # Do the action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            assert reward != -10\n",
    "            reward *= reward_coef\n",
    "            assert reward >= 0\n",
    "            next_state = (next_state[0], next_state[2])\n",
    "            if not done:\n",
    "                # Other player\n",
    "                action_other = eps_greedy_strategy(env, eps, Q)\n",
    "                action_no_other = env.int_from_action(action_other)\n",
    "                next_state_other, reward_other, done_other, _ = env.step(action_other)\n",
    "                assert reward_other != -10\n",
    "                reward = reward_other\n",
    "                reward *= reward_coef\n",
    "                assert reward <= 0\n",
    "                next_state = (next_state_other[0], next_state_other[2])\n",
    "                done = done_other\n",
    "            # Update q_values\n",
    "            td_target = reward + gamma * np.max(Q[next_state])\n",
    "            td_error = td_target - Q[state][action_no]\n",
    "            Q[state][action_no] += learning_rate * td_error\n",
    "            # Update state\n",
    "            state = next_state\n",
    "        pbar.write(f\"Episode {i} finish with reward {reward}.\")\n",
    "        if (i + 1) % (n_episodes // 10) == 0:\n",
    "            my_policy = lambda env: eps_greedy_strategy(env, 0, Q)\n",
    "            quality.append(policy_quality(env, my_policy, n_episodes=n_episodes))\n",
    "            pbar.write(f\"Policy reward vs random policy: {quality[-1]}. Current epsilon: {eps}. \")\n",
    "        pbar.update(1)\n",
    "        sleep(0.001)\n",
    "\n",
    "    plt.plot(list(range(0, n_episodes + 1, n_episodes // 10)), quality)\n",
    "    plt.ylabel(\"Reward vs random policy\")\n",
    "    plt.xlabel(\"Iteration of learning\")\n",
    "    \n",
    "    best_policy = lambda env, Q: eps_greedy_strategy(env, 0, Q)\n",
    "    return best_policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe(n_rows=3, n_cols=3, n_win=3)\n",
    "pi, Q = qlearning(env, n_episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График выше мне кажется очень неплохим.  \n",
    "Оценка качества производилось в игре за крестики. У крестиков на маленьких досках есть явное преимущество, которое даёт первый ход, поэтому уже в начале обучения мы чаще выигрывали случайную стратегию, чем проигрывали ей.  \n",
    "Но уже после 3000 итераций мы научились играть довольно неплохо.  \n",
    "Так же обратите внимание, что я к концу обучения всё таки сделал стратегию более жадной, это позволило увидеть, что обе стороны научились играть одинаково хорошо и почти всегда в конце обучения играли в ничью.  \n",
    "Думаю, что подобрав правило изменения epsilon, мы бы смогли быстрее обучиться до приемлемого качества, наверное, за 2000 итераций можно было бы закончить.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте обучить стратегии крестиков и ноликов для доски 4х4 и/или 5х5.  \n",
    "Disclaimer: начиная с пункта 2, задания для досок размера больше 4х4 могут потребовать большого терпения или более сложных вычислительных реализаций (например, параллелизации). Не хочу в этом задании непременно требовать ни того ни другого, так что если не будет получаться доучить до победных стратегий, не страшно -- но покажите, что со временем что-то всё-таки улучшается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = TicTacToe(n_rows=4, n_cols=4, n_win=4)\n",
    "pi, Q = qlearning(env, n_episodes=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты аналогичны тем, что были выше для доски 3х3, но потребовалось больше итераций обучения, а право первого хода даёт уже не такое большое преимущество, как раньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть вторая: добавим нейронных сетей\n",
    "Реализуйте нейронную сеть для метода DQN на доске для крестиков-ноликов. Не буду ограничивать фантазию, но кажется, что свёртки 3х3 здесь должны неплохо работать (в том числе обобщаться на доски размера побольше).  \n",
    "Реализуйте DQN с нейронной сетью, обучите стратегии крестиков и ноликов. Замечание: скорее всего, experience replay потребуется сразу же.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html  \n",
    "https://mahowald.github.io/pytorch-dqn/  \n",
    "https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/  \n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d  \n",
    "https://neurohive.io/ru/tutorial/cnn-na-pytorch/  \n",
    "https://ai.stackexchange.com/questions/6669/why-does-self-playing-tictactoe-not-become-perfect  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, exptuple):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = exptuple\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "       \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании у нас есть больше свободы: больше параметров модели и процесса обучения, которые нужно подбирать или рассчитывать. Я плотно не занимался подбором параметров, выставлял их на глаз и если что-то не нравилось, корректировал тоже на глаз. Интуитивно действовал. Вообще, относительно того какую сделать модель, как её учить вопросов больше, чем ответов. Поэтому уверен, что при более грамотном подходе можно значительно улучшить количественные показатели, но качественно процесс обучения получилось продемонстрировать.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По рекомендации из условия задачи решил добавить свёрточный слой, а после него поместил два полносвязных слоя.  \n",
    "Я думаю, размеры слоёв должны как-то зависить от размера доски, поэтому попробовал их связать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, field_size=(3, 3), conv_size=3):\n",
    "        nn.Module.__init__(self)\n",
    "        conv_number = int((field_size[0] + field_size[1] + 2) * 1.5)\n",
    "        hidden_layer_size = conv_number * field_size[0] * field_size[1] * 2\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv_number, kernel_size=conv_size, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear((1 + field_size[0] - conv_size) * (1 + field_size[1] - conv_size) * conv_number, hidden_layer_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_layer_size, field_size[0] * field_size[1]),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решил независимо учить две модели за крестики и за нолики. Их можно было бы объединить и аккуратно обучить одну модель, но мне понравилось так.  \n",
    "Я решил сделать побольше исследования новых состояний, поэтому сделал стратегии очень нежадными. Но при этом мне показалось естестенным сохранить хотя бы небольшую динамику изменения epsilon, поэтому я уменьшаю epsilon с 0.95 до 0.85 в процессе обучения. Случайности много, но при этом хорошие ходы будут фигурировать в обучении чаще, чем было бы при использовании абсолютно случайных стратегий.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeDQN():\n",
    "    def __init__(self, n_rows=3, n_cols=3, n_win=3):\n",
    "        self.env = TicTacToe(n_rows, n_cols, n_win)\n",
    "        self.model_x = Network((n_rows, n_cols))\n",
    "        self.model_o = Network((n_rows, n_cols))\n",
    "        self.memory_x = ReplayMemory(1000 * n_rows * n_cols)\n",
    "        self.memory_o = ReplayMemory(1000 * n_rows * n_cols)\n",
    "        self.optimizer_x = optim.Adam(self.model_x.parameters(), 0.001)\n",
    "        self.optimizer_o = optim.Adam(self.model_o.parameters(), 0.001)\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        self.eps_init, self.eps_final, self.eps_decay = 0.95, 0.85, 200\n",
    "        self.eps_threshold = self.eps_init\n",
    "        self.eps_gen = None\n",
    "        self.num_step = 0\n",
    "\n",
    "    def select_greedy_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            values = model(state.expand(1, 1, self.env.n_rows, self.env.n_cols))[0].tolist()\n",
    "            available = [(i, v) for i, v in enumerate(values) if self.env.is_available_action_int(i)]\n",
    "            best_available = list(filter(lambda x: x[1] == max(available, key=lambda x: x[1])[1], available))\n",
    "            best_action = random.choice(best_available)[0]\n",
    "            return torch.tensor([[best_action]])\n",
    "    \n",
    "    def select_random_action(self):\n",
    "        return torch.tensor([[self.env.random_available_action_no()]], dtype=torch.int64)\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            sample = random.random()\n",
    "            self.num_step += 1\n",
    "            if sample > self.eps_threshold:\n",
    "                return self.select_greedy_action(model, state)\n",
    "            else:\n",
    "                return self.select_random_action()\n",
    "    \n",
    "    def state_to_tensor(self, env_state):\n",
    "        state = list(map(int, env_state[0]))\n",
    "        state_tensor = torch.tensor([state], dtype=torch.float32).reshape(self.env.n_rows, self.env.n_cols)\n",
    "        return state_tensor\n",
    "    \n",
    "    def do_step(self, current_state_tensor, greedy=False, random_step=False, render=False):\n",
    "        if self.env.curTurn == 1:\n",
    "            model = self.model_x\n",
    "            if render: print(\"Ходят крестики:\")\n",
    "        else:\n",
    "            model = self.model_o\n",
    "            if render: print(\"Ходят нолики:\")\n",
    "        if random_step:\n",
    "            if render: print(\"Случайный ход\")\n",
    "            action = self.select_random_action()\n",
    "        else:\n",
    "            if greedy:\n",
    "                if render: print(\"Жадная стратегия\")\n",
    "                action = self.select_greedy_action(model, current_state_tensor)\n",
    "            else:\n",
    "                if render: print(\"Почти жадная стратегия\")\n",
    "                action = self.select_action(model, current_state_tensor)\n",
    "        new_state, reward, done, _ = self.env.step(self.env.action_from_int(action.numpy()[0][0]))\n",
    "        if render:\n",
    "            self.env.printBoard()\n",
    "        return action, new_state, reward, done\n",
    "        \n",
    "    \n",
    "    def run_episode(self, e=0, do_learning=True, greedy=False, render=False, vs_random=False, verbose=True, side=\"random\"):\n",
    "        self.model_x.train(False)\n",
    "        self.model_o.train(False)\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        state_tensor = self.state_to_tensor(state)\n",
    "        if render:\n",
    "            self.env.printBoard()\n",
    "        \n",
    "        if do_learning:\n",
    "            if self.eps_gen is None:\n",
    "                self.eps_threshold = self.eps_final + (self.eps_init - self.eps_final) * math.exp(-1. * self.num_step / self.eps_decay)\n",
    "            else:\n",
    "                self.eps_threshold = next(self.eps_gen)\n",
    "        \n",
    "        reward_coef = 1.0\n",
    "        if side == \"o\" or (side == \"random\" and random.random() > 0.5):\n",
    "            if render:\n",
    "                print(\"Играем за нолики\")\n",
    "            _, state, _, _ = self.do_step(state_tensor, greedy=True, random_step=vs_random, render=render)\n",
    "            state_tensor = self.state_to_tensor(state)\n",
    "            reward_coef = -reward_coef\n",
    "        \n",
    "        while True:\n",
    "            action, next_state, reward, done = self.do_step(state_tensor, greedy=greedy, random_step=False, render=render)\n",
    "            next_state_tensor = self.state_to_tensor(next_state)\n",
    "            assert reward != -10\n",
    "            reward *= reward_coef\n",
    "            assert reward >= 0\n",
    "                \n",
    "            if not done:\n",
    "                # Other player\n",
    "                action_other, next_state, reward, done = self.do_step(next_state_tensor, greedy=True, random_step=vs_random, render=render)\n",
    "                next_state_tensor = self.state_to_tensor(next_state)\n",
    "                assert reward != -10\n",
    "                reward *= reward_coef\n",
    "                assert reward <= 0\n",
    "\n",
    "            if do_learning:\n",
    "                transition = (state_tensor, action, next_state_tensor, torch.tensor([reward], dtype=torch.float32))\n",
    "                if reward_coef > 0:\n",
    "                    self.memory_x.store(transition)\n",
    "                else:\n",
    "                    self.memory_o.store(transition)\n",
    "            state = next_state\n",
    "            state_tensor = self.state_to_tensor(state)\n",
    "            \n",
    "            if do_learning:\n",
    "                self.learn()\n",
    "            \n",
    "            if done:\n",
    "                if verbose: print(\"\\tEpisode %d finished with reward %d. Current epsilon == %f.\" % (e, reward, self.eps_threshold))\n",
    "                break\n",
    "            \n",
    "        return reward\n",
    "                \n",
    "    def run(self, n_episodes=10000, render=False, vs_random=False):\n",
    "        if self.eps_gen is None:\n",
    "            self.eps_gen = eps_generator(eps_start=self.eps_init, eps_min=self.eps_final, decay=(self.eps_final / self.eps_init)**(1/n_episodes))\n",
    "        \n",
    "        print(\"%s\\tStarting training for %d episodes...\" % (datetime.now().time(), n_episodes))\n",
    "        quality = []\n",
    "        quality.append(self.mean_reward_vs_random_policy(n_episodes))\n",
    "        print(f\"Current quality == {quality[-1]}\")\n",
    "        for e in range(n_episodes):\n",
    "            self.run_episode(e, render=render, vs_random=vs_random)\n",
    "            if (e + 1) % (n_episodes // 10) == 0:\n",
    "                quality.append(self.mean_reward_vs_random_policy(n_episodes))\n",
    "                print(f\"Current quality == {quality[-1]}\")\n",
    "        print(\"%s\\t\\t...done!\" % (datetime.now().time()))\n",
    "        \n",
    "        plt.plot(list(range(0, n_episodes + 1, n_episodes // 10)), quality)\n",
    "        plt.ylabel(\"Reward vs random policy\")\n",
    "        plt.xlabel(\"Iteration of learning\")\n",
    "        \n",
    "        return quality\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory_x) < self.batch_size or len(self.memory_o) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.model_x.train(True)\n",
    "        self.model_o.train(True)\n",
    "        \n",
    "        x = (self.model_x, self.memory_x, self.optimizer_x)\n",
    "        o = (self.model_o, self.memory_o, self.optimizer_o)\n",
    "        \n",
    "        for model, memory, optimizer in [x, o]:\n",
    "            # берём мини-батч из памяти\n",
    "            transitions = memory.sample(self.batch_size)\n",
    "            batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "            batch_state = Variable(torch.cat([state.expand(1, 1, self.env.n_rows, self.env.n_cols) for state in batch_state]))\n",
    "            batch_action = Variable(torch.cat(batch_action))\n",
    "            batch_reward = Variable(torch.cat(batch_reward))\n",
    "            batch_next_state = Variable(torch.cat([state.expand(1, 1, self.env.n_rows, self.env.n_cols) for state in batch_next_state]))\n",
    "\n",
    "            # считаем значения функции Q\n",
    "            Q = model(batch_state).gather(1, batch_action).reshape([self.batch_size])\n",
    "\n",
    "            # оцениваем ожидаемые значения после этого действия\n",
    "            Qmax = model(batch_next_state).detach().max(1)[0]\n",
    "            Qnext = batch_reward + (self.gamma * Qmax)\n",
    "\n",
    "            # и хотим, чтобы Q было похоже на Qnext -- это и есть суть Q-обучения\n",
    "            loss = F.smooth_l1_loss(Q, Qnext)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def mean_reward_vs_random_policy(self, n_episodes=1000, side=\"x\"):\n",
    "        self.model_x.train(False)\n",
    "        self.model_o.train(False)\n",
    "        rewards = 0\n",
    "        for i in range(n_episodes):\n",
    "            rewards += self.run_episode(e=0, do_learning=False, greedy=True, render=False, vs_random=True, verbose=False, side=side)\n",
    "        return rewards / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(1)\n",
    "dqn = TicTacToeDQN(3, 3, 3)\n",
    "quality = dqn.run(10000, vs_random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну... График не очень красивый, но видно, что результат от обучения есть: мы играем значительно лучше, чем играет случайная стратегия.  \n",
    "Смотреть на вознаграждения в процессе обучения здесь не имеет большого смысла, потому что стратегии совершают очень много случайных действий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для доски побольше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(1)\n",
    "dqn = TicTacToeDQN(4, 4, 4)\n",
    "quality = dqn.run(10000, vs_random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять не очень красиво, и, похоже, можно было продолжать ещё учить, но эффект от обучения виден и на этом этапе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте Double DQN и/или Dueling DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/double-deep-q-networks-905dd8325412  \n",
    "https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/  \n",
    "https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/  \n",
    "https://en.wikipedia.org/wiki/Q-learning#Double_Q-learning  \n",
    "https://medium.com/@leosimmons/double-dqn-implementation-to-solve-openai-gyms-cartpole-v-0-df554cd0614d  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь ничего не придумывал, скопировал код для DQN и изменил функцию обучения. Реализован один из самых простых вариантов DoubleDQN (кажется, вариант реализации, представленный в первой публикации о DoubleDQN): веса второй модели подтягивают к первой жёстко, но не на каждой итерации. Позже были предложены более красивые реализации с мягким подтягиванием весов одной модели к другой, но я решил попробовать простой вариант, и он неплохо сработал, я его таким и оставил."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeDoubleDQN():\n",
    "    def __init__(self, n_rows=3, n_cols=3, n_win=3):\n",
    "        self.env = TicTacToe(n_rows, n_cols, n_win)\n",
    "        self.models_x = [Network((n_rows, n_cols)), Network((n_rows, n_cols))]\n",
    "        self.models_o = [Network((n_rows, n_cols)), Network((n_rows, n_cols))]\n",
    "        self.memory_x = ReplayMemory(1000 * n_rows * n_cols)\n",
    "        self.memory_o = ReplayMemory(1000 * n_rows * n_cols)\n",
    "        learning_rate = 0.001\n",
    "        self.optimizers_x = [optim.Adam(self.models_x[0].parameters(), learning_rate), \n",
    "                             optim.Adam(self.models_x[1].parameters(), learning_rate)]\n",
    "        self.optimizers_o = [optim.Adam(self.models_o[0].parameters(), learning_rate), \n",
    "                             optim.Adam(self.models_o[1].parameters(), learning_rate)]\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        self.eps_init, self.eps_final, self.eps_decay = 0.95, 0.85, 200\n",
    "        self.eps_threshold = self.eps_init\n",
    "        self.eps_gen = None\n",
    "        self.num_step = 0\n",
    "        self.update_count = 0\n",
    "\n",
    "    def select_greedy_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            values = model(state.expand(1, 1, self.env.n_rows, self.env.n_cols))[0].tolist()\n",
    "            available = [(i, v) for i, v in enumerate(values) if self.env.is_available_action_int(i)]\n",
    "            best_available = list(filter(lambda x: x[1] == max(available, key=lambda x: x[1])[1], available))\n",
    "            best_action = random.choice(best_available)[0]\n",
    "            return torch.tensor([[best_action]])\n",
    "    \n",
    "    def select_random_action(self):\n",
    "        return torch.tensor([[self.env.random_available_action_no()]], dtype=torch.int64)\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            sample = random.random()\n",
    "            self.num_step += 1\n",
    "            if sample > self.eps_threshold:\n",
    "                return self.select_greedy_action(model, state)\n",
    "            else:\n",
    "                return self.select_random_action()\n",
    "    \n",
    "    def state_to_tensor(self, env_state):\n",
    "        state = list(map(int, env_state[0]))\n",
    "        state_tensor = torch.tensor([state], dtype=torch.float32).reshape(self.env.n_rows, self.env.n_cols)\n",
    "        # state_tensor = (state_tensor + 1.0) * self.env.curTurn\n",
    "        return state_tensor\n",
    "    \n",
    "    def do_step(self, current_state_tensor, greedy=False, random_step=False, render=False):\n",
    "        if self.env.curTurn == 1:\n",
    "            model = self.models_x[0]\n",
    "            if render: print(\"Ходят крестики:\")\n",
    "        else:\n",
    "            model = self.models_o[0]\n",
    "            if render: print(\"Ходят нолики:\")\n",
    "        if random_step:\n",
    "            if render: print(\"Случайный ход\")\n",
    "            action = self.select_random_action()\n",
    "        else:\n",
    "            if greedy:\n",
    "                if render: print(\"Жадная стратегия\")\n",
    "                action = self.select_greedy_action(model, current_state_tensor)\n",
    "            else:\n",
    "                if render: print(\"Почти жадная стратегия\")\n",
    "                action = self.select_action(model, current_state_tensor)\n",
    "        new_state, reward, done, _ = self.env.step(self.env.action_from_int(action.numpy()[0][0]))\n",
    "        if render:\n",
    "            self.env.printBoard()\n",
    "        return action, new_state, reward, done\n",
    "        \n",
    "    \n",
    "    def run_episode(self, e=0, do_learning=True, greedy=False, render=False, vs_random=False, verbose=True, side=\"random\"):\n",
    "        self.models_x[0].train(False)\n",
    "        self.models_x[1].train(False)\n",
    "        self.models_o[0].train(False)\n",
    "        self.models_o[1].train(False)\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        state_tensor = self.state_to_tensor(state)\n",
    "        if render:\n",
    "            self.env.printBoard()\n",
    "        \n",
    "        if do_learning:\n",
    "            if self.eps_gen is None:\n",
    "                self.eps_threshold = self.eps_final + (self.eps_init - self.eps_final) * math.exp(-1. * self.num_step / self.eps_decay)\n",
    "            else:\n",
    "                self.eps_threshold = next(self.eps_gen)\n",
    "        \n",
    "        reward_coef = 1.0\n",
    "        if side == \"o\" or (side == \"random\" and random.random() > 0.5):\n",
    "            if render:\n",
    "                print(\"Играем за нолики\")\n",
    "            _, state, _, _ = self.do_step(state_tensor, greedy=True, random_step=vs_random, render=render)\n",
    "            state_tensor = self.state_to_tensor(state)\n",
    "            reward_coef = -reward_coef\n",
    "        \n",
    "        while True:\n",
    "            action, next_state, reward, done = self.do_step(state_tensor, greedy=greedy, random_step=False, render=render)\n",
    "            next_state_tensor = self.state_to_tensor(next_state)\n",
    "            assert reward != -10\n",
    "            reward *= reward_coef\n",
    "            assert reward >= 0\n",
    "                \n",
    "            if not done:\n",
    "                # Other player\n",
    "                action_other, next_state, reward, done = self.do_step(next_state_tensor, greedy=True, random_step=vs_random, render=render)\n",
    "                next_state_tensor = self.state_to_tensor(next_state)\n",
    "                assert reward != -10\n",
    "                reward *= reward_coef\n",
    "                assert reward <= 0\n",
    "\n",
    "            if do_learning:\n",
    "                transition = (state_tensor, action, next_state_tensor, torch.tensor([reward], dtype=torch.float32))\n",
    "                if reward_coef > 0:\n",
    "                    self.memory_x.store(transition)\n",
    "                else:\n",
    "                    self.memory_o.store(transition)\n",
    "            state = next_state\n",
    "            state_tensor = self.state_to_tensor(state)\n",
    "            \n",
    "            if do_learning:\n",
    "                self.learn()\n",
    "            \n",
    "            if done:\n",
    "                if verbose: print(\"\\tEpisode %d finished with reward %d. Current epsilon == %f.\" % (e, reward, self.eps_threshold))\n",
    "                break\n",
    "            \n",
    "        return reward\n",
    "                \n",
    "    def run(self, n_episodes=10000, render=False, vs_random=False):\n",
    "        if self.eps_gen is None:\n",
    "            self.eps_gen = eps_generator(eps_start=self.eps_init, eps_min=self.eps_final, decay=(self.eps_final / self.eps_init)**(1/n_episodes))\n",
    "        \n",
    "        print(\"%s\\tStarting training for %d episodes...\" % (datetime.now().time(), n_episodes))\n",
    "        quality = []\n",
    "        quality.append(self.mean_reward_vs_random_policy(n_episodes))\n",
    "        print(f\"Current quality == {quality[-1]}\")\n",
    "        for e in range(n_episodes):\n",
    "            self.run_episode(e, render=render, vs_random=vs_random)\n",
    "            if (e + 1) % (n_episodes // 10) == 0:\n",
    "                quality.append(self.mean_reward_vs_random_policy(n_episodes))\n",
    "                print(f\"Current quality == {quality[-1]}\")\n",
    "        print(\"%s\\t\\t...done!\" % (datetime.now().time()))\n",
    "        \n",
    "        plt.plot(list(range(0, n_episodes + 1, n_episodes // 10)), quality)\n",
    "        plt.ylabel(\"Reward vs random policy\")\n",
    "        plt.xlabel(\"Iteration of learning\")\n",
    "        \n",
    "        return quality\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.update_count += 1\n",
    "        if self.update_count % 5 == 0:\n",
    "            self.models_x[1].load_state_dict(self.models_x[0].state_dict())\n",
    "            self.models_x[1].load_state_dict(self.models_x[0].state_dict())\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.memory_x) < self.batch_size or len(self.memory_o) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.models_x[0].train(True)\n",
    "        self.models_x[1].train(True)\n",
    "        self.models_o[0].train(True)\n",
    "        self.models_o[1].train(True)\n",
    "        \n",
    "        x = (self.models_x, self.memory_x, self.optimizers_x)\n",
    "        o = (self.models_o, self.memory_o, self.optimizers_o)\n",
    "        \n",
    "        for models, memory, optimizers in [x, o]:\n",
    "            # берём мини-батч из памяти\n",
    "            transitions = memory.sample(self.batch_size)\n",
    "            batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "            batch_state = Variable(torch.cat([state.expand(1, 1, self.env.n_rows, self.env.n_cols) for state in batch_state]))\n",
    "            batch_action = Variable(torch.cat(batch_action))\n",
    "            batch_reward = Variable(torch.cat(batch_reward))\n",
    "            batch_next_state = Variable(torch.cat([state.expand(1, 1, self.env.n_rows, self.env.n_cols) for state in batch_next_state]))\n",
    "\n",
    "            # считаем значения функции Q\n",
    "            Q = models[0](batch_state).gather(1, batch_action).reshape([self.batch_size])\n",
    "\n",
    "            # оцениваем ожидаемые значения после этого действия\n",
    "            Qmax = models[0](batch_next_state).detach()[:, torch.argmax(models[1](batch_next_state).detach(), 1)][: ,0]\n",
    "            Qnext = batch_reward + (self.gamma * Qmax)\n",
    "            \n",
    "            loss = F.smooth_l1_loss(Q, Qnext)\n",
    "            optimizers[0].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[0].step()\n",
    "            \n",
    "            self.update_target_network()\n",
    "\n",
    "    def mean_reward_vs_random_policy(self, n_episodes=1000, side=\"x\"):\n",
    "        self.models_x[0].train(False)\n",
    "        self.models_x[1].train(False)\n",
    "        self.models_o[0].train(False)\n",
    "        self.models_o[1].train(False)\n",
    "        rewards = 0\n",
    "        for i in range(n_episodes):\n",
    "            rewards += self.run_episode(e=0, do_learning=False, greedy=True, render=False, vs_random=True, verbose=False, side=side)\n",
    "        return rewards / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "ddqn3 = TicTacToeDoubleDQN(3, 3, 3)\n",
    "quality = ddqn3.run(10000)\n",
    "print(quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта кривая значительно красивее, чем та, что была для DQN. Видно, что процесс обучения стал стабильнее, а то же качество может быть достигнуто за меньшее число итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "ddqn4 = TicTacToeDoubleDQN(4, 4, 4)\n",
    "quality = ddqn4.run(10000)\n",
    "print(quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для доски 4х4: стало хорошо. Такой график уже радует. Обучается быстрее и стабильнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть третья: расширим и углубим поиск\n",
    "Крестики-нолики -- это, конечно, далеко не го, и обычный альфа-бета поиск с отсечением здесь наверняка может работать идеально вплоть до довольно больших досок. Однако мы всё-таки для этого учебного задания будем реализовывать более практически релевантный метод MCTS -- заодно фактически получится и упражнение на многоруких бандитов.  \n",
    "Реализуйте rollouts со случайной стратегией и (опционально) rollouts с неслучайной, но простой стратегией (например, основанной на дополнении нескольких паттернов или на Q-функции, которая у вас получилась в первом пункте).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://robotics.stackexchange.com/questions/16596/what-is-the-definition-of-rollout-in-neural-network-or-openai-gym  \n",
    "https://arxiv.org/pdf/1910.00120.pdf  \n",
    "https://towardsdatascience.com/monte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f  \n",
    "https://towardsdatascience.com/monte-carlo-tree-search-implementing-reinforcement-learning-in-real-time-game-player-25b6f6ac3b43  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая проигрывает партию с текущего состояния до конца в соответствии с заданными стратегиями игры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, x_policy, o_policy, my_side='x', smart_reward=False, render=False):\n",
    "    policies = [None, x_policy, o_policy]  # для удобства индексации с помощью curTurn\n",
    "    reward = 0\n",
    "    done = env.gameOver\n",
    "    if render: env.printBoard()\n",
    "    while not done:\n",
    "        _, reward, done, _ = env.step(policies[env.curTurn](env))\n",
    "        if render: env.printBoard()\n",
    "        assert reward != -10\n",
    "    if smart_reward and my_side == 'o':\n",
    "        reward = -reward\n",
    "    return reward\n",
    "\n",
    "def random_policy(env):\n",
    "    return env.random_available_action()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сыграем несколько партий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe(3, 3, 3)\n",
    "\n",
    "env.reset()\n",
    "print(rollout(env, random_policy, random_policy))\n",
    "\n",
    "env.reset()\n",
    "print(rollout(env, random_policy, random_policy))\n",
    "\n",
    "env.reset()\n",
    "print(rollout(env, random_policy, random_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь попробуем играть, проигрывая в уме продолжение партии с помощью ранее написанной функции, и таким образом выбирая наилучшие действия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала без проигрывания партий в уме (играет случайная стратегия, не имеющая представления, как лучше ходить, против обученной DoubleDQN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn3_policy_x(env):\n",
    "    ddqn3.env = env\n",
    "    return env.action_from_int(ddqn3.select_greedy_action(ddqn3.models_x[0], ddqn3.state_to_tensor(env.getState())).numpy()[0][0])\n",
    "    \n",
    "def mean_reward_without_rollout(n_episodes=100):\n",
    "    env = TicTacToe(3, 3, 3)\n",
    "    cum_rewards = 0\n",
    "    for _ in range(n_episodes):\n",
    "        env.reset()\n",
    "        cum_rewards += rollout(env, ddqn3_policy_x, random_policy)\n",
    "    return cum_rewards / n_episodes\n",
    "\n",
    "print(mean_reward_without_rollout())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что обученная ранее стратегия играет лучше нашей случайной.  \n",
    "А теперь сделаем следующее: у нас нет информации о том, какой ход лучше, но есть информация о том, как противник отвечает на наши ходы, т.е. мы имеем возможность проиграть партию в уме для всех доступных действий. Сделаем это.  \n",
    "Сейчас наша стратегия станет жёсткой: мы будем однозначно и закономерно выбирать ходы. И стратегия противника жёсткая. Поэтому играть много партий не нужно - они будут одинаковые. Сыграем одну: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_policy(env):\n",
    "    available_actions = env.getEmptySpaces()\n",
    "    best_action = None\n",
    "    best_reward = -10\n",
    "    for action in available_actions:\n",
    "        env_copy = copy.deepcopy(env)\n",
    "        env_copy.step(action)\n",
    "        rollout_reward = rollout(env_copy, ddqn3_policy_x, rollout_policy, 'o', True)\n",
    "        if rollout_reward > best_reward:\n",
    "            best_reward = rollout_reward\n",
    "            best_action = action\n",
    "    return best_action\n",
    "\n",
    "def mean_reward_with_rollout(n_episodes=100, render=False):\n",
    "    env = TicTacToe(3, 3, 3)\n",
    "    cum_rewards = 0\n",
    "    for _ in tq.tqdm(range(n_episodes)):\n",
    "        env.reset()\n",
    "        env.step(random_policy(env))\n",
    "        cum_rewards += rollout(env, ddqn3_policy_x, rollout_policy, render=render)\n",
    "    return cum_rewards / n_episodes\n",
    "\n",
    "print(mean_reward_with_rollout(1, render=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что имея возможность проигрывать продолжение партий в уме, нолики выиграли."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте MCTS-поиск с этими rollouts для крестиков-ноликов на досках разного размера, сравните полученные стратегии между собой и со стратегиями, обученными в первых двух частях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(s, position, character):\n",
    "    return s[:position] + character + s[position+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.8\n",
    "\n",
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.parent = None\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "        self.state = None\n",
    "        self.index = 0\n",
    "        self.storage = None\n",
    "    \n",
    "    def ucb(self):\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        return self.value / self.visits + C * math.sqrt(math.log(self.storage[self.parent].visits) / self.visits)\n",
    "    \n",
    "    def create_children(self):\n",
    "        if self.is_leaf():\n",
    "            np_state = np.array(list(map(int, self.state)))\n",
    "            available_actions = np.where(np_state == 1)[0]\n",
    "            if (len(self.state) - len(available_actions)) % 2 == 0:\n",
    "                curTurn = 1\n",
    "            else:\n",
    "                curTurn = -1\n",
    "            for action in available_actions:\n",
    "                child = Node()\n",
    "                child.parent = self.index\n",
    "                child.state = replace(self.state, action, str(curTurn + 1))\n",
    "                child.index = len(self.storage)\n",
    "                child.storage = self.storage\n",
    "                self.children[action] = child.index\n",
    "                self.storage.append(child)\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    def select_child(self, how=\"best\"):\n",
    "        eps = 0.7\n",
    "        if how == \"best\":\n",
    "            fun = max\n",
    "            characteristic = lambda x: self.storage[x[1]].ucb()\n",
    "        elif how == \"worse\":\n",
    "            fun = min\n",
    "            characteristic = lambda x: self.storage[x[1]].ucb()\n",
    "        elif how == \"random\":\n",
    "            return random.choice(list(self.children.items()))\n",
    "        elif how == \"eps_greedy_best\":\n",
    "            if random.random() < eps:\n",
    "                return random.choice(list(self.children.items()))\n",
    "            else:\n",
    "                fun = max\n",
    "                characteristic = lambda x: self.storage[x[1]].ucb()\n",
    "        elif how == \"eps_greedy_worse\":\n",
    "            if random.random() < eps:\n",
    "                return random.choice(list(self.children.items()))\n",
    "            else:\n",
    "                fun = min\n",
    "                characteristic = lambda x: self.storage[x[1]].ucb()  \n",
    "        else:\n",
    "            fun = None\n",
    "            characteristic = None\n",
    "        if not self.is_leaf():\n",
    "            return max(self.children.items(), key=characteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mcts():\n",
    "    def __init__(self, n_rows=3, n_cols=3, n_win=3):\n",
    "        self.env = TicTacToe(n_rows, n_cols, n_win)\n",
    "        self.env.reset()\n",
    "\n",
    "        self.nodes = []\n",
    "\n",
    "        root = Node()\n",
    "        root.state = self.env.getState()[0]\n",
    "        root.index = 0\n",
    "        root.storage = self.nodes\n",
    "        self.nodes.append(root)\n",
    "        \n",
    "        self.current_node = 0\n",
    "        \n",
    "    def next_step(self):\n",
    "        hows = [None, \"best\", \"worse\"]\n",
    "        action, self.current_node = self.nodes[self.current_node].select_child(hows[self.env.curTurn])\n",
    "        self.env.step(self.env.action_from_int(action))\n",
    "    \n",
    "    def selection(self):\n",
    "        while not self.nodes[self.current_node].is_leaf():\n",
    "            self.next_step()\n",
    "    \n",
    "    def expansion(self):\n",
    "        if not self.env.gameOver:\n",
    "            if self.nodes[self.current_node].visits > 0:\n",
    "                self.nodes[self.current_node].create_children()\n",
    "                self.next_step()\n",
    "    \n",
    "    def simulation(self):\n",
    "        return rollout(copy.deepcopy(self.env), random_policy, random_policy)\n",
    "    \n",
    "    def backpropagation(self, r):\n",
    "        while self.nodes[self.current_node].parent is not None:\n",
    "            self.nodes[self.current_node].visits += 1\n",
    "            self.nodes[self.current_node].value += r\n",
    "            self.current_node = self.nodes[self.current_node].parent\n",
    "        self.nodes[self.current_node].visits += 1\n",
    "        self.nodes[self.current_node].value += r\n",
    "    \n",
    "    def learn(self, n_episodes=100000):\n",
    "        for _ in tq.tqdm(range(n_episodes)):\n",
    "            assert self.current_node == 0\n",
    "            self.env.reset()\n",
    "            self.selection()\n",
    "            self.expansion()\n",
    "            r = self.simulation()\n",
    "            self.backpropagation(r)\n",
    "            \n",
    "    def play_greedy_game(self):\n",
    "        self.env.reset()\n",
    "        self.env.printBoard()\n",
    "        self.current_node = 0\n",
    "        hows = [None, \"best\", \"worse\"]\n",
    "        while not self.nodes[self.current_node].is_leaf():\n",
    "            action, self.current_node = self.nodes[self.current_node].select_child(hows[self.env.curTurn])\n",
    "            self.env.step(self.env.action_from_int(action))\n",
    "            self.env.printBoard()\n",
    "    \n",
    "    def mean_reward_vs_random_policy(self, n_episodes=1000, side='x'):\n",
    "        if side == 'x':\n",
    "            hows = [None, \"best\", \"random\"]\n",
    "        elif side == 'o':\n",
    "            hows = [None, \"random\", \"worse\"]\n",
    "        else:\n",
    "            hows = None\n",
    "        cum_rewards = 0\n",
    "        count_games = 0\n",
    "        for _ in range(n_episodes):\n",
    "            self.env.reset()\n",
    "            self.current_node = 0\n",
    "            while not self.nodes[self.current_node].is_leaf():\n",
    "                action, self.current_node = self.nodes[self.current_node].select_child(hows[self.env.curTurn])\n",
    "                _, reward, done, _ = self.env.step(self.env.action_from_int(action))\n",
    "            if done:\n",
    "                cum_rewards += reward\n",
    "                count_games += 1\n",
    "        mean_reward = cum_rewards / count_games\n",
    "        print(f\"Mean reward vs random policy equal {mean_reward} after {count_games} games.\")\n",
    "        return mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовали построение дерева. Попробуем построить и посмотреть результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts3 = Mcts(3, 3, 3)\n",
    "mcts3.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на партию жадных стратегий, пользующихся построенным деревом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcts3.play_greedy_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы строили дерево недолго, некоторые его листы могут не быть терминальными состояниями в игре. Т.е. может получиться так, что до листа дерева мы прошли,а игра ещё не закончилась. В оценке среднего выигрыша я такие партии игнорировал и брал для оценки только законченные партии. Оценим средний выигрыш против случайной стратегии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts3.mean_reward_vs_random_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично для 4х4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts4 = Mcts(4, 4, 4)\n",
    "mcts4.learn(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts4.play_greedy_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts4.mean_reward_vs_random_policy(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты 4х4 вызывают некоторые сомнения. Слишком мало игр доигрались до конца.  \n",
    "Попробую найти этому объяснение. \n",
    "* Когда алгоритм выбирает какой-то путь в дереве, он запускает на этом пути rollout. Происходит игра двух случайных стратегий, чтобы определить, к какому результату может привести этот путь. Игра 4х4 достаточно длинная и сложная, чтобы случайная стратегия выигрывала в неё у другой случайной стратегии крайне редко. Поэтому все новые пути, которые даже могли быть хорошими, кажутся алгоритму примерно одинаково неважными. И лишь иногда так получается, что случайные симуляции выделяют какой-то путь на фоне остальных, сыграв несколько показательных партий по этому направлению. После этого алгоритм начинает сильнее углубляться по этим путям и уделять меньше внимания остальным путям в дереве. А т.к. игра 4х4 достаточно длинная (дерево глубокое), то на путях, которым уделяется недостаточно внимания конец игры достигнут будет очень нескоро.  \n",
    "* Максимальное количество узлов в дереве меняется примерно как факториал количества клеток на доске. Это очень много, начиная с досок размеров 4х4.  \n",
    "\n",
    "Получаем, что из 1000 партий завершились не многие. Но завершились победой )  \n",
    "Что с этим можно сделать, как сделать результаты красивее?  \n",
    "Кажется, что если строить дерево значительно дольше, то мы должны исследовать больше путей в дереве, но я пробовал учить несколько часов, это не сильно помогло, а учить ещё дольше в рамках домашней работы не очень хочется, хочется предложить качественное, а не количественное решение. Я пробовал менять условия выбора действия (делать их менее жадными), но всё это слабо меняет результат, потому что дерево очень большое. Можно попробовать выстраивать линию не из 4 символов, а из трёх, тогда игра будет заканчиваться раньше. Посмотрим..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts4short = Mcts(4, 4, 3)\n",
    "mcts4short.learn(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts4short.play_greedy_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts4short.mean_reward_vs_random_policy(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже лучше. По такому количеству игр уже можно делать выводы. Думаю, пока я на этом остановлюсь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество игры примерно такое же, как то, которое мы получали, учась другими методами, это похоже на правду.  \n",
    "\n",
    "Что я понял об MCTS:  \n",
    "```\n",
    "+ Просто реализовать\n",
    "+ Достаточно знать лишь правила игры\n",
    "- Требователен к ресурсам\n",
    "- Для сложных игр использовать mcts не просто\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
